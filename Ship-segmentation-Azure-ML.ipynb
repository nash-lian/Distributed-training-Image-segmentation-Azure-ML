{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9380833a1a2503c5d3518f0ed8d6df8dcf05b7c2"
   },
   "source": [
    "## Overview\n",
    "The whole processing has 2 steps:\n",
    "1. Image classification: classifying images with or without ships. \n",
    "2. Image segmentation: segmenting ships from images.\n",
    "We downsample images into 256 X 256. However, the downsampling caused ship size to be only 1 pixel, which leads to lower segmentation performance. So we select images with larger ship size for the segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a2f9181ed56a8310f6188ac1254f903574fb115"
   },
   "outputs": [],
   "source": [
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks.hooks import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "from azureml.core import Workspace, Datastore, Dataset, Experiment, Run, Environment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "from azureml.train.dnn import PyTorch, Mpi\n",
    "from azureml.train.hyperdrive import GridParameterSampling\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.train.hyperdrive import HyperDriveConfig\n",
    "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.train.hyperdrive import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\n",
    "\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fa09c99d9f5b03e8e3a213f6d84902d5e1d59e1"
   },
   "source": [
    "### Prepare Azure Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect the workspace\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "\n",
    "subscription_id = '<Your Azure subscription id>'\n",
    "resource_group = '<Your resource group in Azure'\n",
    "workspace_name = '<Your workspace name>'\n",
    "\n",
    "workspace = Workspace(subscription_id=subscription_id, resource_group=resource_group, workspace_name=workspace_name,\n",
    "                      auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register storage container as datastore\n",
    "storange_name = '<Your Azure storage name>'\n",
    "ket_to_storage = '<Key to your storage>'\n",
    "datastore_name = 'airbus'\n",
    "\n",
    "datastore = Datastore.register_azure_blob_container(workspace=workspace, \n",
    "                                                    datastore_name=datastore_name, \n",
    "                                                    container_name=datastore_name,\n",
    "                                                    account_name=storange_name, \n",
    "                                                    account_key=ket_to_storage,\n",
    "                                                    create_if_not_exists=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find datastore by name\n",
    "datastore = Datastore.get(workspace, datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect/create computer resource\n",
    "cluster_name = 'gpu-nc24'\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace = workspace, name = cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size = 'STANDARD_NC24', min_nodes = 0, max_nodes = 4)\n",
    "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output = True, min_node_count = 4, timeout_in_minutes = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register dataset\n",
    "dataset = Dataset.File.from_files(path=(datastore, 'airbus'))\n",
    "dataset = dataset.register(workspace=workspace,\n",
    "                           name='Airbus root',\n",
    "                           description='Dataset for airbus images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the script folder\n",
    "script_folder = os.path.join(os.getcwd(), \"training_scripts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clean\n",
    "1. downsize images to 256 X 256 \n",
    "2. Put images to 2 folders: ship or no ship\n",
    "3. Create the segmentation label images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment to clear data\n",
    "exp_data = Experiment(workspace = workspace, name = 'urthecast_data_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register data reference\n",
    "data_folder = DataReference(\n",
    "    datastore=datastore,\n",
    "    data_reference_name=\"airbus_root\",\n",
    "    path_on_datastore = 'airbus',\n",
    "    mode = 'mount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator for data clean\n",
    "script_params = {\n",
    "    '--data_folder': data_folder\n",
    "}\n",
    "est_data = PyTorch(source_directory = script_folder,\n",
    "                    compute_target = compute_target,\n",
    "                    entry_script = 'clean-data.py',  # python script for cleaning\n",
    "                    script_params = script_params,\n",
    "                    use_gpu = False,\n",
    "                    node_count=1,\n",
    "                    pip_packages = ['fastai'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit for running\n",
    "data_run = exp_data.submit(est_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show run details\n",
    "RunDetails(data_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ship/No ship classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment to classification\n",
    "exp_class = Experiment(workspace = workspace, name = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data reference for classification data\n",
    "class_data_folder = DataReference(\n",
    "    datastore=datastore,\n",
    "    data_reference_name=\"airbus_class\",\n",
    "    path_on_datastore = 'airbus/class',\n",
    "    mode = 'mount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator for classification\n",
    "from azureml.train.dnn import PyTorch, Mpi\n",
    "\n",
    "script_params = {\n",
    "    '--data_folder': class_data_folder,\n",
    "    '--num_epochs': 5\n",
    "}\n",
    "\n",
    "est_class = PyTorch(source_directory = script_folder,\n",
    "                    compute_target = compute_target,\n",
    "                    entry_script = 'classification.py', # Classification script\n",
    "                    script_params = script_params,\n",
    "                    use_gpu = True,\n",
    "                    node_count=3,                       # 3 nodes are used\n",
    "                    distributed_training=Mpi(process_count_per_node = 4), # 4 GPU's per node\n",
    "                    pip_packages = ['fastai'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyper drive for parameter tunning\n",
    "param_sampling = GridParameterSampling({\n",
    "    'start_learning_rate': choice(0.0001, 0.001),\n",
    "    'end_learning_rate': choice(0.01, 0.1)})\n",
    "\n",
    "hyperdrive_class = HyperDriveConfig(estimator = est_class,\n",
    "                                         hyperparameter_sampling = param_sampling,\n",
    "                                         policy = None,\n",
    "                                         primary_metric_name = 'dice',\n",
    "                                         primary_metric_goal = PrimaryMetricGoal.MAXIMIZE,\n",
    "                                         max_total_runs = 4,\n",
    "                                         max_concurrent_runs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick off running\n",
    "classification_run = exp_class.submit(hyperdrive_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show running details\n",
    "RunDetails(classification_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results for all running\n",
    "classification_run.wait_for_completion(show_output = False)\n",
    "\n",
    "children = list(classification_run.get_children())\n",
    "metricslist = {}\n",
    "i = 0\n",
    "\n",
    "for single_run in children:\n",
    "    results = {k: np.min(v) for k, v in single_run.get_metrics().items() if (k in ['dice', 'loss']) and isinstance(v, float)}\n",
    "    parameters = single_run.get_details()['runDefinition']['arguments']\n",
    "    try:\n",
    "        results['start_learning_rate'] = parameters[5]\n",
    "        results['end_learning_rate'] = parameters[7]\n",
    "        metricslist[i] = results\n",
    "        i += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "rundata = pd.DataFrame(metricslist).sort_index(1).T.sort_values(by = ['loss'], ascending = True)\n",
    "rundata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best running\n",
    "best_run = classification_run.get_best_run_by_primary_metric()\n",
    "best_run.get_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f487dd77687f4edb070bd5d2dc9da9a001d62bdb"
   },
   "source": [
    "### Ship segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data reference for segmentation\n",
    "sgmt_data_folder = DataReference(\n",
    "    datastore=datastore,\n",
    "    data_reference_name=\"airbus_segmentation\",\n",
    "    path_on_datastore = 'airbus/segmentation',\n",
    "    mode = 'mount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment for segmentation\n",
    "exp_sgmt = Experiment(workspace = workspace, name = 'segmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56ed39146115a4767a257fec60a3b367284fa0d6"
   },
   "outputs": [],
   "source": [
    "# Estimator for segmentation\n",
    "segmt_script_params = {\n",
    "    '--data_folder': sgmt_data_folder,\n",
    "    '--img_folder': '256-filter99',\n",
    "    '--num_epochs': 12\n",
    "}\n",
    "\n",
    "segmt_est = PyTorch(source_directory = script_folder,\n",
    "                    compute_target = compute_target,\n",
    "                    entry_script = 'segmentation.py', # Segmentation script\n",
    "                    script_params = segmt_script_params,\n",
    "                    use_gpu = True,\n",
    "                    node_count=4,                     # 4 nodes\n",
    "                    distributed_training=Mpi(process_count_per_node = 4), # 4 GPU's per node\n",
    "                    pip_packages = ['fastai'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "25fa3283c992696575914a5fdb6ebc433a0b5d1f"
   },
   "outputs": [],
   "source": [
    "# Kick off running\n",
    "segmentation_run = exp_sgmt.submit(config=segmt_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running detail\n",
    "RunDetails(segmentation_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "193381699f5595c916647bfd6c51eaeba699379d"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "segmentation_run.wait_for_completion(show_output=False)  # specify True for a verbose log\n",
    "print(segmentation_run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register model\n",
    "model = larger_sgmt_run.register_model(model_name='segmentation-99',\n",
    "                           tags={'ship': 'min99'},\n",
    "                           model_path='outputs/segmentation.pkl')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a5af78f6512ab4f514818ef47b3481ef67a65e46"
   },
   "source": [
    "### Prediction\n",
    "Sample code for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "size = 256\n",
    "ifile = '<Test image>'\n",
    "img = open_image(ifile)\n",
    "img = img.resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "model_path = '<The model path>'\n",
    "learn = load_learner(model_path)\n",
    "pred = learn.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
